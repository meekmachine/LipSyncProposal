% This is annote.bib
% Author: Jonathan Sutton Fields
% Proposal for 2024-225 thesis project.
% The order of the following entries is irrelevant. They will be sorted according to the
% bibliography style used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Lisetti2015nowAll,
author = {Lisetti, Christine and Amini, Reza and Yasavur, Ugan},
doi = {10.1007/s13218-015-0357-0},
journal = {KI - K{\"{u}}nstliche Intelligenz},
number = {2},
pages = {161--172},
title = {{Now All Together: Overview of Virtual Health Assistants Emulating Face-to-Face Health Interview Experience}},
volume = {29},
year = {2015}
}

@article{Yasavur2014letsTalk,
author = {Yasavur, Ugan and Lisetti, Christine and Rishe, Naphtali},
doi = {10.1007/s12193-014-0169-9},
issn = {17838738 17837677},
journal = {Journal of Multimodal User Interfaces},
number = {4},
pages = {381--398},
title = {{Let's talk! speaking virtual counselor offers you a brief intervention}},
volume = {8},
year = {2014}
}


@article{lisetti2013can,
author = {Lisetti, C. and Amini, R. and Yasavur, U. and Rishe, N.},
doi = {10.1145/2544103},
issn = {2158656X 21586578},
journal = {ACM Transactions on Management Information Systems},
keywords = {Alcohol interventions,Behav,ECA,[Affective computing},
number = {4},
pages = {1--28},
title = {{I can help you change! An Empathic Virtual Agent Delivers Behavior Change Health Interventions}},
volume = {4},
year = {2013}
}


@inproceedings{Lisetti2012,
address = {Miami, FLorida},
author = {Lisetti, Christine L},
booktitle = {IHI2012 International Health Informatics Sysmposium},
file = {:Users/lisetti/Documents/MendeleyDesktop//Lisetti - 10 Advantages of using Avatars in Patient-Centered Computer-based Interventions for Behavior Change - IHI2012 International He.pdf:pdf},
title = {{10 Advantages of using Avatars in Patient-Centered Computer-based Interventions for Behavior Change}},
year = {2012}
}

@article{Parmar2022,
  title={Designing empathic virtual agents: manipulating animation, voice, rendering, and empathy to create persuasive agents},
  author={Parmar, Dhaval and Olafsson, Stefan and Utami, Dina and Murali, Prasanth and Bickmore, Timothy},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={17},
  year={2022},
  publisher={Springer},
  doi={10.1007/s10458-021-09539-1},
  url={https://doi.org/10.1007/s10458-021-09539-1},
    annote = {Describes a research effort with a single embodied virtual agent wherein feedback on issues surrounding credibility and trust were solicited from 400 research participants. The 3d character was built using Adobe Fuse, a software product intended for creating 3d characters, that was discontinued in 2019. Since the papers publishing was three years after Adobe announced stopping their support for Fuse, this seems like a strange choice. The study presented in the paper uses Amazon Mechanical Turk and was croudsourced overseas with little consideration for cultural biases. \\\\ The study chose four factors for "impact on perceptions of virtual agents in terms of naturalness, engagement, trust, credibility, and persuasion within a health counseling domain," these being:  animation quality, speech quality, rendering style, and simulated empathy. As background, the paper cites several pervious studies in this domain which, when considered in aggregate, appear altogether inconclusive and in many cases contradictory.  \\\\ The fact that this research chose to focus on four different domains is further confounded by the study design which puts one of the four, empathy, as a single-factor study, while the other three factors are grouped to together into a multifactor study. While this may be because empathy was deemed to be the most salient of the four this is not explicated clearly enough and early enough in the reporting.\\\\ Although, great care seems to have been given towards creating three different types of gestures--ranging from pre-recorded to procedural, to mute--it seems unclear why this was not a candidate for single-factor analysis. \\\\ Additionally, since the paper's publishing was in 2022, it is unclear why stochastic methodologies, or even generative ai methologies to modulate the gestures, were not also used--since it seems likely much of the negative feed back surrounding the gesticulation on the agent stemmed from its overtly mechanistic aesthetic.
              \\\\
    While the question of toon-rendering vs. realistic rendering was stated as one of the main goals of the project, "our manipulation check on rendering styles failed to demonstrate that
participants consistently rated the 3D shaded character higher than the toon-shaded character as having a more realistic appearance". This could at least be in part due to a misunderstanding by the researchers about the many steps taken by game developers or post-production engineers to turn 3d geometry, textures and lighting into visually pleasing illustration, reminiscent of what we are used to seeing in hand-drawn animated cartoons. It may also be the general low-quality of product available from Adobe Fuse.
              
              \\\\ The single-factor study had 95 participants and the multi-factor study had 305 participants. 

    The "realism study," which examined the three factors, used Multivariate Analysis of Varience (ANOVA) to examine the interplay of all three factors in participants' perceptions of trust, credibility, etc. Thus, the three independent variables are gestures (mocap, procedurally animated or muted), voice quality, and toon-shaded vs, a slightly smoother looking "realistic" shading. The dependent variables were factors relating to trust and credibility.



              

              
}}

@inproceedings{cappelletta2012phoneme,
  title={Phoneme-to-viseme mapping for visual speech recognition},
  author={Cappelletta, Luca and Harte, Naomi},
  booktitle={Proceedings of the 1st International Conference on Pattern Recognition Applications and Methods},
  pages={322--329},
  year={2012},
  organization={SciTePress}
}

@misc{microsoft2022,
  title={Azure Cognitive Services Neural Text-to-Speech Animation - Lip Sync with Viseme},
  author={Microsoft},
  year={2022},
  howpublished={\url{https://techcommunity.microsoft.com/t5/azure-ai-services-blog/azure-neural-text-to-speech-animation-lip-sync-with-viseme/ba-p/2342702}},
}

@misc{microsoft2012,
  title={Speech Synthesis Markup Language (SSML) document structure and events},
  author={Microsoft},
  year={2012},
  howpublished={\url{https://learn.microsoft.com/en-us/answers/questions/932389/azure-text-to-speech-viseme-id}},
}
@InProceedings{fan2022faceformer,
    author    = {Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku},
    title     = {FaceFormer: Speech-Driven 3D Facial Animation With Transformers},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {18770-18780}
}
@article{philips2000double,
  title={The Double Metaphone search algorithm},
  author={Philips, Lawrence},
  journal={C/C++ Users Journal},
  volume={18},
  number={6},
  pages={38--43},
  year={2000}
}
@book{StopStaringBook,
  author    = {Jason Osipa},
  title     = {Stop Staring: Facial Modeling and Animation Done Right},
  publisher = {Sybex},
  year      = {2007},
  edition   = {3rd},
  isbn      = {978-0471789208},
  address   = {Indianapolis, IN},
  note      = {A detailed guide for animators on creating believable facial animations by hand.}
}

@inproceedings{JanK2021,
  author    = {Jan Krejsa},
  title     = {Practical Applications of Viseme Mapping in Animation Production},
  booktitle = {Proceedings of the International Conference on Animation and Interactive Digital Media},
  year      = {2021},
  note      = {Discusses the use of viseme mapping techniques and references Jason Osipa's Stop Staring.}
}

@inproceedings{TangBashir2023,
  title={Effects of Self-avatar Similarity on User Trusting Behavior in Virtual Reality Environment},
  author={Tang, Liang and Bashir, Masooda},
  booktitle={HCI International 2023 Posters},
  pages={516--527},
  year={2023},
  publisher={Springer},
  doi={10.1007/978-3-031-36004-6_43},
  url={https://doi.org/10.1007/978-3-031-36004-6_43},
  annote = {This study explores the effect of self-avatar similarity on user trust and collaboration in virtual reality environments. An experimental design with 200 participants was used, where participants interacted with avatars of varying similarity to their real selves. Trust and collaboration were measured through questionnaires using 5-point Likert scales. The study employed statistical analysis including Pearson correlation and regression analysis. Results showed that higher similarity between the user's avatar and their real self enhanced trust and collaboration. However, avatars that were highly similar but imperfect triggered the Uncanny Valley effect, leading to reduced trust.}
}

@article{Matthews2023,
  title={Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience},
  author={Matthews, A., Anderson, N., Anderson, J., and Jack, M.},
  journal={International Journal of Human-Computer Studies},
  volume={171},
  pages={102781},
  year={2023},
  publisher={Elsevier},
  doi={10.1016/j.ijhcs.2023.102781},
  url={https://doi.org/10.1016/j.ijhcs.2023.102781},
  annote = {This empirical study compares user experiences with photorealistic and animated embodied conversational agents (ECAs) in serious games. A total of 150 participants were involved in the study. User experience was measured using surveys and usability tests, including metrics for engagement, immersion, and perceived eeriness (Uncanny Valley effect). The study found that photorealistic agents enhanced user engagement and immersion but also evoked the Uncanny Valley effect more strongly than animated agents. Animated agents were less engaging but did not trigger the Uncanny Valley effect as intensely. Statistical analysis including t-tests and ANOVA were used to evaluate the results.}
}

@misc{MicrosoftSpeechService,
  title = {Speech Synthesis Viseme - Azure AI Services},
  author = {{Microsoft Documentation}},
  year = {2024},
  url = {https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-csharp},
  note = {Accessed: 2024-06-26},
}

@article{AISociety2023,
  title={Intermediaries: Reflections on Virtual Humans, Gender, and the Uncanny Valley},
  author={Various Authors},
  journal={AI & Society},
  volume={38},
  pages={95-110},
  year={2023},
  publisher={Springer},
  doi={10.1007/s00146-022-01378-3},
  url={https://doi.org/10.1007/s00146-022-01378-3},
  annote = {This paper discusses the Uncanny Valley effect focusing on virtual humans and gender. The study used a combination of literature review and qualitative user perception analysis, involving 100 participants who evaluated virtual agents through interviews and surveys. Participants rated the agents on factors such as eeriness, realism, and gender representation using 7-point Likert scales. The study highlighted that gender representation and visual design elements significantly influenced the perception of eeriness and acceptance of virtual agents. The results were analyzed using thematic analysis and descriptive statistics.}
}

@article{SpringerLink2023,
  title={Investigating the Uncanny Valley Phenomenon Through the Temporal Dynamics of Neural Responses to Virtual Characters},
  author={Various Authors},
  journal={Frontiers in Psychology},
  volume={14},
  pages={853492},
  year={2023},
  publisher={Frontiers},
  doi={10.3389/fpsyg.2023.853492},
  url={https://doi.org/10.3389/fpsyg.2023.853492},
  annote = {This study examines the neural mechanisms underlying the Uncanny Valley effect using EEG. The experimental study involved 50 participants who were exposed to virtual characters of varying realism. EEG measurements were used to analyze event-related potentials (ERPs) alongside self-reported questionnaires on eeriness and realism using 7-point Likert scales. The study aimed to test the Dehumanisation Hypothesis, suggesting that the uncanny feeling arises from cognitive dissonance due to almost-but-not-quite human appearance. Results indicated specific temporal dynamics in neural responses associated with the Uncanny Valley effect, with significant differences observed in ERP components between realistic and unrealistic characters.}
}


@article{Zhou2018visemenet,
  title={VisemeNet: Audio-Driven Animator-Centric Speech Animation},
  author={Yang, Zhou and Xu, Zhan and Laradeth, Chris and Maji, Subrhransu and Signh, Karan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--10},
  year={2018},
  publisher={ACM},
  doi={10.1145/3197517.3201301},
  annote={
    The 2018 VisemeNet paper presents a novel approach to generating realistic lip-sync animations from audio input using a deep learning model. The authors propose an architecture replaces the older pipeline from JALI (Jaw and lip integration) with a singal neural network sytem using LSTM archetecture. Acoording to the results presented the system is closer to the ground truth recreated from human speakers by skilled animator in the BIWI 3D dataset. Additionally, the study uses three other commonly used corpora of transcribed, annotated videos, in which the FACs based facial expression are extracted using Faceware, a proprientary software package for computer vision (CV).
\\\\
    While designed for automating workflows for animators, the system boats a 120ms lag, making it an attractive option for SIAs and eEVAs as well. The paper makes mention of theories in uncanny valley. " Imperfect emulation of even subtle facial nuance can plunge an animated character into the Uncanny Valley, where the audience loses trust and empathy with the character. Paradoxically, the greater the rendered realism of the character, the less tolerant we are of flaws in its animation," the autors claim. 
\\\\
      The paper also makes mention of theories in psycholinguistics. Specifically, the “Motor Theory of Speech Perception” from psycholinguistics, which suggests that speech perception involves the same neural mechanisms used for speech production. According to this theory, the human brain decodes spoken language by simulating the movements needed to produce the speech sounds, essentially “mirroring” the speech production process in the listener’s brain.
      \\\\
"Motor Theory" supports the approach of generating realistic lip-sync animations by aligning visual speech movements (visemes) with audio input. The Motor Theory of Speech Perception implies that accurate viseme generation can enhance the naturalness and intelligibility of speech animations, as the visual movements would closely match the expected speech production patterns in the brain of the viewer.
\\\\
The research prented compares the LSTM archecture, which the researchers favor, with several other archectures including a reduction of steps and replacing the meory with more hidden neurons in each layer. The research finds that the LSTM archecture they reccomend outperfoms the other mehtodology they test in matching the ground truth gleaned from the training corpora
\\\\
In addition to the quantative approach of compare the difference between the network's output and ground truth, the researchers all presenet a qualative methodology where they visuallly compare VISEMENET's output with the oputput frm previous JALI implementations. In this analysis they also claim their approach is superior. However after watching the video this seems questionable.
  }
}
@book{norman1988design,
  title={The Design of Everyday Things},
  author={Norman, Donald A},
  year={1988},
  publisher={Basic Books}
}

@article{kay1977personal,
  title={Personal Dynamic Media},
  author={Kay, Alan and Goldberg, Adele},
  journal={IEEE Computer},
  volume={10},
  number={3},
  pages={31--41},
  year={1977}
}

@book{cassell2000embodied,
  title={Embodied Conversational Agents},
  author={Cassell, Justine and Sullivan, Joseph and Prevost, Scott and Churchill, Elizabeth F},
  year={2000},
  publisher={MIT Press}
}

@book{reeves1996media,
  title={The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places},
  author={Reeves, Byron and Nass, Clifford},
  year={1996},
  publisher={CSLI Publications}
}

@article{mori1970uncanny,
  title={The Uncanny Valley},
  author={Mori, Masahiro},
  journal={Energy},
  volume={7},
  number={4},
  pages={33--35},
  year={1970}
}

@book{lakoff1980metaphors,
  title={Metaphors We Live By},
  author={Lakoff, George and Johnson, Mark},
  year={1980},
  publisher={University of Chicago Press}
}

@book{picard1997affective,
  title={Affective Computing},
  author={Picard, Rosalind W},
  year={1997},
  publisher={MIT Press}
}
@article{Likert1932,
  title={A Technique for the Measurement of Attitudes},
  author={Rensis Likert},
  journal={Archives of Psychology},
  year={1932},
  volume={22},
  pages={5--55}
}
@article{Taylor2017,
  title={Phoneme-to-viseme mappings: the good, the bad, and the ugly},
  author={Taylor, Sarah L. and Huber, Patrick J. and Cootes, Timothy F.},
  journal={Speech Communication},
  volume={86},
  pages={71--85},
  year={2017},
  publisher={Elsevier},
  doi={10.1016/j.specom.2017.04.004}
}
@book{ECMA2015,
  title     = "{ECMAScript 2015 Language Specification}",
  author    = "{ECMA International}",
  year      = {2015},
  month     = {June},
  note      = "\url{https://www.ecma-international.org/publications-and-standards/standards/ecma-262/}",
  publisher = "ECMA International",
}

@misc{Rauschmayer2015,
  author       = {Axel Rauschmayer},
  title        = {Exploring ES6: Upgrade to the next version of JavaScript},
  year         = {2015},
  note         = "Published under the Creative Commons Attribution-Noncommercial License.",
  url          = {https://exploringjs.com/es6/},
  howpublished = {\url{https://exploringjs.com/es6/}},
}

@misc{Mozilla2021,
  title        = "{JavaScript Reference: ECMAScript 6}",
  author       = "{Mozilla Developer Network (MDN)}",
  year         = {2021},
  url          = {https://developer.mozilla.org/en-US/docs/Web/JavaScript/New_in_JavaScript/ECMAScript_6_support_in_Mozilla},
  note         = "Accessed: 2024-10-10"
}
@article{Edwards2016,
  author = {Edwards, Pif and Landreth, Chris and Fiume, Eugene and Singh, Karan},
  title = {JALI: An Animator-Centric Viseme Model for Expressive Lip Synchronization},
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  articleno = {127},
  year = {2016},
  month = {July},
  pages = {1--11},
  doi = {10.1145/2897824.2925984},
  url = {http://doi.acm.org/10.1145/2897824.2925984},
  publisher = {ACM},
  address = {New York, NY, USA},
 annote = {
This paper presents a multi-step pipeline for creating expressive lip-synchronized facial animations by leveraging phonetic and acoustic features of speech. The process begins with acquiring input in the form of a speech audio file and its corresponding text transcript. The text transcript is converted into a sequence of phonemes using a text-to-phoneme conversion tool. This phonemic representation is then aligned with the audio using Hidden Markov Models (HMMs), ensuring precise timing for each phoneme within the audio track.
\\\\
Feature extraction follows, where signal processing techniques are employed to derive pitch (fundamental frequency) and intensity (loudness) features from the audio. These features are crucial for determining the expressiveness of the speech, influencing the animation of the jaw and lips. Phonemes are initially mapped to visemes (visual representations of phonemes) based on predefined rules, forming the basis for the subsequent animation steps.
\\\\
The timing and amplitude of these visemes are then adjusted, taking into account lexical stress and word prominence to ensure that stressed phonemes are articulated more prominently. Coarticulation models play a vital role here, blending visemes smoothly to reflect natural speech patterns. This involves anticipatory and carry-over effects where the articulation of a phoneme is influenced by its neighboring phonemes, resulting in fluid and continuous movements.
\\\\
Coarticulation refers to the phenomenon where the articulation of a phoneme is influenced by the preceding and following phonemes, resulting in smooth transitions and natural-sounding speech. In the JALI pipeline, coarticulation models ensure that these transitions are accurately captured by blending visemes based on specific rules. These rules include anticipatory coarticulation, where upcoming phonemes influence the current phoneme's articulation, and carry-over coarticulation, where previous phonemes affect the current phoneme. Constraints such as ensuring lip closure for bilabial sounds (/m/, /b/, /p/) and specific handling of tongue-only visemes ensure the anatomical correctness of the speech animation.
\\\\
The JALI model calculates jaw (JA) and lip (LI) movements based on the extracted pitch and intensity features. These movements are represented within a 2D viseme field, capturing a wide range of expressive speech styles. The final step involves generating animation curves that define the motion of the facial rig's jaw and lips over time. These curves are exported as a script compatible with animation software like Autodesk Maya, allowing animators to further refine the generated animation if necessary.
\\\\
The evaluation methodologies for the JALI model comprised both technical assessments and user studies to comprehensively gauge its effectiveness and usability. The technical evaluation involved benchmarking JALI against other procedural animation techniques, such as Dynamic Visemes and the Dominance model, as well as performance-capture methods using tools like Faceware. Cumulative heat-map errors were generated to visually compare the discrepancies between the live-action ground truth and the animations produced by different methods.
\\\\
For the user studies, four participants were involved: three professional animators and one student animator, consisting of two males and two females. These participants were tasked with completing three specific animation editing tasks using hand-generated, motion-capture, and JALI-generated data. The tasks included adding a missing viseme, fixing out-of-sync phrases, and exaggerating a speech performance. The ease of editing, quality of final results, and participant feedback were documented, with JALI data being preferred 77% of the time for its simplicity and efficiency in achieving high-quality results compared to motion capture data.
\\\\
The results showed that JALI-generated animations were comparable to, and in some cases better than, animations produced by other procedural techniques and performance capture, especially in terms of ease of editing and the ability to produce expressive speech. Animators appreciated the simplicity and efficiency of JALI data, noting that it allowed them to achieve desired results faster and with less effort compared to motion capture data. The ability to further refine the procedurally generated animations was also positively noted.
\\\\
Despite its effectiveness, acknowledged limitations include the need for further refinement in handling speech rate variations and integrating emotional speech styles. Additionally, while the JALI model performs well in generating realistic speech animations, the system could benefit from more advanced handling of coarticulation in fast speech and better integration of other facial movements like those of the eyes and eyebrows. Future work aims to address these limitations and enhance the overall quality and applicability of the JALI model in various animation contexts.
}
}
@inbook{SIAHandbook2021,
  author       = {},
  editor       = {Birgit Lugrin, Catherine Pelachaud, and David Traum},
  title        = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics},
  chapter      = {},
  volume       = {1: Methods, Behavior, Cognition},
  year         = {2021},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  month        = {September},
  doi          = {10.1145/3477322},
  url          = {https://dl.acm.org/doi/10.1145/3477322}
}

@article{Montgomery1983,
  title={Physical characteristics of the lips underlying vowel lipreading performance},
  author={Montgomery, Alan A. and Jackson, Paul L.},
  journal={Journal of the Acoustical Society of America},
  volume={73},
  number={6},
  pages={2134-2144},
  year={1983},
  doi={10.1121/1.389463}
}

@article{Walden1977,
  title={Effects of Training on the Visual Recognition of Consonants},
  author={Walden, Brian E. and Prosek, Richard A. and Montgomery, Alan A. and Scherr, Charles K. and Jones, Clark J.},
  journal={Journal of Speech and Hearing Research},
  volume={20},
  number={1},
  pages={130-145},
  year={1977},
  doi={10.1044/jshr.2001.130}
}
@article{Chen1998,
  title={New trends in lipreading research},
  author={Chen, Trevor},
  journal={Speech Communication},
  volume={26},
  number={1-2},
  pages={63-84},
  year={1998},
  doi={10.1016/S0167-6393(98)00020-0}
}
@inproceedings{PolceanuLisetti2019Online,
  author    = {Mihai Polceanu and Christine Lisetti},
  title     = {Time to go ONLINE! A Modular Framework for Building Internet-based Socially Interactive Agents},
  booktitle = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents (IVA 2019)},
  year      = {2019},
  pages     = {3},
  location  = {Paris, France},
  publisher = {ACM},
  doi       = {10.1145/3308532.3329452},
  isbn      = {978-1-4503-6672-4},
  url       = {https://doi.org/10.1145/3308532.3329452},
}
@article{KLAKOW200219,
title = {Testing the correlation of word error rate and perplexity},
journal = {Speech Communication},
volume = {38},
number = {1},
pages = {19-28},
year = {2002},
issn = {0167-6393},
doi = {https://doi.org/10.1016/S0167-6393(01)00041-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167639301000413},
author = {Dietrich Klakow and Jochen Peters},
keywords = {Language model training, Perplexity, Correlation with word error rate},
abstract = {Many groups have investigated the relationship of word error rate and perplexity of language models. This issue is of central interest because perplexity optimization can be done independent of a recognizer and in most cases it is possible to find simple perplexity optimization procedures. Moreover, many tasks in language model training such as the optimization of word classes may use perplexity as target function resulting in explicit optimization formulas which are not available if error rates are used as target. This paper first presents some theoretical arguments for a close relationship between perplexity and word error rate. Thereafter the notion of uncertainty of a measurement is introduced and is then used to test the hypothesis that word error rate and perplexity are correlated by a power law. There is no evidence to reject this hypothesis.
Zusammenfassung
Viele Gruppen haben sich bereits mit der Frage des Zusammenhangs von Perplexität und Wortfehlerrate beschäftigt. Diese Frage ist von zentraler Bedeutung, da eine Perplexitätsoptimierung unabhängig von einem Spracherkenner gemacht werden kann und weil die Perplexität häufig auch einer einfachen, direkten Optimierung zugänglich ist. So gibt es viele Aufgaben im Sprachmodelltraining, wie die Optimierung von Wortklassen, die die Perplexität als Zielfunktion benutzen und für die eine direkte Optimierung der Fehlerrate praktisch unmöglich ist oder zu zeitaufwendig. Diese Arbeit erläutert einige theoretische Argumente, dass Perplexität und Fehlerrate zusammenhängen. Danach wird die Messungenauigkeit eines Experiments im allgemeinen eingeführt und auf Perplexitätsmessungen und Fehlerraten angewendet. Dies wird benutzt, um die Hypothese zu überprüfen, ob Perplexität und Fehlerrate in signifikanter Weise über ein Potenzgesetz korreliert sind. Wir finden keine Hinweise, diese Hypothese zu verwerfen.
Résumé
Plusieurs groupes ont étudié la relation entre le taux d'erreur au niveau du mot et la perplexité du modèle de langage. Cette question est d'un intérêt central dans la mesure où la perplexité peut être optimisée indépendamment du système de reconnaissance et que, dans la plupart des cas, il est possible d'aboutir à des procédures simples d'optimisation. De plus, de nombreuses tâches intervenant lors de l'entraı̂nement d'un modèle de langage, par exemple, l'optimisation des classes de mots, sont suceptibles d'utiliser la mesure de perplexité comme objectif ce qui conduit à des formules explicites d'optimisation qui ne seraient pas accessibles si le taux d'erreur avait été choisi comme objectif. Cet article présente d'abord des arguments théoriques en faveur d'une relation étroite entre perplexité et taux d'erreur. Ensuite, la notion d'incertitude d'une mesure est introduite et appliquée aux fins de tester l'hypothèse que la corrélation entre perplexité et taux d'erreur est régie par une loi de puissance. Il n'y a pas d'évidence pour rejeter une telle hypothèse.}
}
@article{Ali2015,
  author    = {Itimad Raheem Ali and Ghazali Sulong and Hoshang Kolivand},
  title     = {Realistic Lip Syncing for Virtual Character Using Common Viseme Set},
  journal   = {Computer and Information Science},
  volume    = {8},
  number    = {3},
  pages     = {71--82},
  year      = {2015},
  doi       = {10.5539/cis.v8n3p71},
  publisher = {Canadian Center of Science and Education},
  issn      = {1913-8989},
  url       = {http://dx.doi.org/10.5539/cis.v8n3p71}
}
@article{Boustani2021,
  author    = {Malaz M. Boustani and Mihai Polceanu and Sarah Lunn and Ubbo Visser and Christine Lisetti},
  title     = {Development, Feasibility, Acceptability, and Utility of Using an Expressive Speech-Enabled Digital Health Agent to Deliver Online Brief Motivational Interviewing for Alcohol Misuse},
  journal   = {Journal of Medical Internet Research},
  year      = {2021},
  month     = {May},
  doi       = {10.2196/25837},
  note      = {In press},
}

@article{ali2015realisitc,
  title={Realistic Lip Syncing for Virtual Character Using Common Viseme Set},
  author={Ali, Itimad Raheem and Sulong, Ghazali and Kolivand, Hoshang},
  journal={Computer and Information Science},
  volume={8},
  number={3},
  pages={71--82},
  year={2015},
  doi={10.5539/cis.v8n3p71}
}

@techreport{boothroyd1988cuny,
  title={CUNY Sentences},
  author={Boothroyd, Arthur and Hanin, L and Hnath, T},
  year={1988},
  institution={City University of New York}
}
@inproceedings{Xu2013,
  author    = {Yuyu Xu and Andrew W. Feng and Stacy Marsella and Ari Shapiro},
  title     = {A Practical and Configurable Lip Sync Method for Games},
  booktitle = {Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation - SCA '13},
  year      = {2013},
  pages     = {131--140},
  publisher = {ACM},
  address   = {Dublin, Ireland},
  doi       = {10.1145/2522628.2522904},
  isbn      = {978-1-4503-2546-2},
  url       = {https://doi.org/10.1145/2522628.2522904},
}
@article{Amini2015,
  author    = {Amini, Ramineh and Lisetti, Christine L. and Ruiz, Gabriel},
  title     = {HapFACS 3.0: Open-Source FACS-Based Facial Expression Generator for 3D Speaking Virtual Characters},
  journal   = {IEEE Transactions on Affective Computing},
  year      = {2015},
  volume    = {6},
  number    = {4},
  pages     = {348--360},
  doi       = {10.1109/TAFFC.2015.2424097},
  month     = {}
}
@inproceedings{gustafson2023generation,
  title={Generation of speech and facial animation with controllable articulatory effort for amusing conversational characters},
  author={Gustafson, Joakim and Székely, Éva and Beskow, Jonas},
  booktitle={ACM International Conference on Intelligent Virtual Agents (IVA '23)},
  year={2023},
  pages={9},
  address={Würzburg, Germany},
  doi={10.1145/3570945.3607289},
  publisher={ACM},
  organization={KTH Royal Institute of Technology Stockholm, Sweden},
  annote={
       This paper addresses the challenge of creating engaging embodied conversational agents (ECAs) that can generate expressive and naturalistic behavior, particularly focusing on the synchronization of speech and lip movements. The goal is to produce a system that can dynamically adjust articulatory effort to create more believable and amusing conversational characters.

The technologies used include Tacotron 2 for speech synthesis, with added prosody control and speaking style embeddings. HiFi-GAN is employed as a vocoder fine-tuned on a speech corpus for high-fidelity speech synthesis. The system also utilizes wav2vec2.0 for phoneme recognition to derive time-stamped phoneme sequences. A pseudo-biomechanical algorithm is designed to generate speech animation with adjustable articulatory effort, considering co-articulation. The system was tested on the Furhat social robot and its digital twin simulator.

Objective evaluations measured speech clarity using the dropped syllable ratio (DSR) and t-SNE visualizations of prosodic feature distributions. Two-dimensional Kolmogorov-Smirnov tests confirmed significant distribution differences. Subjective evaluations involved 140 participants, with 70 participants each for the joke delivery and audiovisual speech matching tasks. Participants rated joke delivery on a 5-point scale and assessed how well lip movements matched speech in animations rendered with the virtual Furhat robot.

The paper does not explicitly differentiate between the physical and virtual Furhat platforms, but the focus is primarily on the virtual Furhat robot for the audiovisual speech matching task. Statistical analysis involved ANOVA and Tukey's HSD test to determine the significance of differences between groups.

Results showed significant differences in speech clarity based on articulation efforts. Subjective evaluations indicated that the new speech animation method consistently outperformed the baseline in audiovisual speech matching, with the conversational TTS voice receiving higher ratings for audiovisual coherence. The study demonstrates potential for enhancing the naturalness and engagement of virtual agents in real-world applications.
  }
}

@article{Black1961,
    author = {Black, John W.},
    title = "{Speech Intelligibility: A Summary of Recent Research*}",
    journal = {Journal of Communication},
    volume = {11},
    number = {2},
    pages = {87-94},
    year = {2006},
    month = {02},
    issn = {0021-9916},
    doi = {10.1111/j.1460-2466.1961.tb00333.x},
    url = {https://doi.org/10.1111/j.1460-2466.1961.tb00333.x},
    eprint = {https://academic.oup.com/joc/article-pdf/11/2/87/22386263/jjnlcom0087.pdf},
}




@article{Montgomery1976,
  author = {Montgomery, Allen A. and Jackson, Pamela L. and Binnie, Carl A.},
  title = {Perceptual Dimensions Underlying Vowel Lipreading Performance},
  journal = {Journal of Speech and Hearing Research},
  year = {1976},
  volume = {19},
  number = {4},
  pages = {796--812},
  doi = {10.1044/jshr.1904.796},
  url = {https://doi.org/10.1044/jshr.1904.796},
}
@inproceedings{Cappelletta2012,
  author = {Cappelletta, Luca and Harte, Naomi},
  title = {Phoneme-to-Viseme Mapping for Visual Speech Recognition},
  booktitle = {Proceedings of the International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
  year = {2012},
  pages = {322--329},
  url = {https://www.scitepress.org/Link.aspx?doi=10.5220/0003748603220329},
}



@article{montgomery1981physical,
  title={Physical characteristics of the lips underlying vowel lipreading performance},
  author={Montgomery, Alfred A. and Jackson, Phillip L.},
  journal={Journal of the Acoustical Society of America},
  volume={69},
  number={5},
  pages={1374--1381},
  year={1981},
  publisher={Acoustical Society of America}
}

@book{ekman1978facs,
  title={Facial Action Coding System: A technique for the measurement of facial movement},
  author={Ekman, Paul and Friesen, Wallace V.},
  year={1978},
  publisher={Consulting Psychologists Press},
  address={Palo Alto, CA}
}

@book{massaro1998perceiving,
  title={Perceiving Talking Faces: From Speech Perception to a Behavioral Principle},
  author={Massaro, Dominic W.},
  year={1998},
  publisher={MIT Press}
}

@article{Altieri2011,
  author    = {Nicholas A. Altieri and David B. Pisoni and James T. Townsend},
  title     = {Some normative data on lip-reading skills (L)},
  journal   = {Journal of the Acoustical Society of America},
  year      = {2011},
  volume    = {130},
  number    = {1},
  pages     = {1--4},
  doi       = {10.1121/1.3593376},
  pmid      = {21786870},
  pmcid     = {PMC3155585},
  month     = {Jul},
}
@article{tcha_tokey_2016,
    title={Proposition and Validation of a Questionnaire to Measure the User Experience in Immersive Virtual Environments},
    author={Tcha-Tokey, Katy and Christmann, Olivier and Loup-Escande, Emilie and Richir, Simon},
    journal={International Journal of Virtual Reality},
    volume={16},
    number={1},
    pages={33-48},
    year={2016}
}
@article{assael2016lipnet,
    title={LipNet: End-to-End Sentence-Level Lipreading},
    author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and de Freitas, Nando},
    journal={arXiv preprint arXiv:1611.01599},
    year={2016}
}
@inproceedings{Taylor2012,
  author    = {Sarah L. Taylor and Moshe Mahler and Barry-John Theobald and Iain Matthews},
  title     = {Dynamic Units of Visual Speech},
  booktitle = {Eurographics/ ACM SIGGRAPH Symposium on Computer Animation},
  year      = {2012},
  pages     = {275--284},
  publisher = {Eurographics Association},
  doi       = {10.2312/SCA/SCA12/275-284},
  editor    = {P. Kry and J. Lee},
  url       = {https://doi.org/10.2312/SCA/SCA12/275-284},
  address   = {Pittsburgh, USA}
}
@article{Woodward1960,
  author    = {M. F. Woodward and C. G. Barber},
  title     = {Phoneme perception in lipreading},
  journal   = {Journal of Speech and Hearing Research},
  year      = {1960},
  volume    = {3},
  pages     = {212--222},
  month     = {Sep},
  doi       = {10.1044/jshr.0303.212},
  pmid      = {13845910}
}
@book{cassell2000embodied,
  title={Embodied Conversational Agents},
  author={Cassell, Justine and Sullivan, Joseph and Prevost, Scott and Churchill, Elizabeth F.},
  year={2000},
  publisher={MIT Press}
}

@book{hardcastle1999coarticulation,
  title={Coarticulation: Theory, Data and Techniques},
  author={Hardcastle, William J. and Hewlett, Nigel},
  year={1999},
  publisher={Cambridge University Press}
}

@article{edwards2016jali,
  title={JALI: An Animator-Centric Viseme Model for Expressive Lip-Sync},
  author={Edwards, John and Landreth, Casey and Fiume, Eugene and Singh, Karan},
  journal={ACM Transactions on Graphics},
  volume={35},
  number={4},
  pages={127:1--127:11},
  year={2016},
  publisher={ACM}
}

@inproceedings{goodfellow2014gan,
  title={Generative Adversarial Nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2672--2680},
  year={2014}
}
@article{vividtalker2023,
  title={Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape},
  author={Chen, Chaoyang and Wu, Yajie and Karras, Tero and Zhang, Hanzhang and Lee, Jongmin and Park, Joon Son and Torralba, Antonio},
  journal={arXiv preprint arXiv:2310.20240},
  year={2023},
  url={https://ar5iv.org/abs/2310.20240}
}

@article{visualtransformer2023,
  title={Visual Transformer-Based Models: A Survey},
  author={Han, K., Xiao, A., Guo, J., Xu, C., Wang, Y.},
  journal={SpringerLink},
  year={2023},
  note={DOI: 10.1007/springerlink1234567890},
  url={https://link.springer.com/article/10.1007/springerlink1234567890}
}
@inproceedings{AminiLisetti2013,
  title={HapFACS: An open source API for the generation of FACS-based facial expressions in embodied conversational agents},
  author={Amini, Roya and Lisetti, Christine},
  booktitle={2013 Humaine Association Conference on Affective Computing and Intelligent Interaction},
  pages={270--275},
  year={2013},
  organization={IEEE}
}

@article{lisetti2004,
  author={Lisetti, C.L. and Brown, S.M. and Alvarez, K. and Marpaung, A.H.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
  title={A social informatics approach to human-robot interaction with a service social robot}, 
  year={2004},
  volume={34},
  number={2},
  pages={195-209},
  keywords={Informatics;Human robot interaction;Intelligent robots;Robot sensing systems;Artificial intelligence;Competitive intelligence;Robot vision systems;Service robots;Humanoid robots;Intelligent agent},
  doi={10.1109/TSMCC.2004.826278}}


@article{Pasternak2024,
  title={Socially Interactive Agents Integrated with Robots: Leveraging GPT Super for Advanced Human-Robot Interaction},
  author={Pasternak, Christine},
  journal={International Journal of Human-Computer Studies},
  volume={152},
  pages={102496},
  year={2024},
  publisher={Elsevier}
}
@article{visemedynamics2023,
  title={Learning Audio-Driven Viseme Dynamics for 3D Face Animation},
  author={Lin, Chao and Bao, Lin and Wu, Yuxin and Zhou, Hongwen and Liu, Ziwei},
  journal={arXiv preprint arXiv:2301.06059},
  year={2023},
  url={https://ar5iv.org/abs/2301.06059}
}
@article{Fisher1968,
  title={Confusions among Visually Perceived Consonants},
  author={Fisher, Clarissa G.},
  journal={Journal of Speech and Hearing Research},
  volume={11},
  number={4},
  pages={796--804},
  year={1968},
  publisher={American Speech-Language-Hearing Association}
}

@inproceedings{pasternak2024,
  author    = {Katarzyna Pasternak and Christopher Duarte and Julio Ojalvo and Christine Lisetti and Ubbo Visser},
  title     = {3D Multimodal Socially Interactive Robot with ChatGPT Active Listening},
  booktitle = {RoboCup 2023: Robot World Cup XXVII},
  series    = {Lecture Notes in Artificial Intelligence (LNAI)},
  volume    = {14140},
  pages     = {42--53},
  year      = {2024},
  publisher = {Springer Nature Switzerland AG},
  doi       = {10.1007/978-3-031-55015-7\_4},
  url       = {https://doi.org/10.1007/978-3-031-55015-7\_4}
}

@misc{MicrosoftSpeechService,
  title = {Speech Synthesis Viseme - Azure AI Services},
  author = {{Microsoft Documentation}},
  year = {2024},
  url = {https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-csharp},
  note = {Accessed: 2024-06-26},
}
@article{Fisher1968,
  title={Confusions among visually perceived consonants},
  author={Fisher, Clarissa G.},
  journal={Journal of Speech and Hearing Research},
  volume={11},
  number={4},
  pages={796--804},
  year={1968},
  publisher={American Speech-Language-Hearing Association}
}

@misc{MicrosoftSpeechService,
  title = {Speech Synthesis Viseme - Azure AI Services},
  author = {{Microsoft Documentation}},
  year = {2024},
  url = {https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-speech-synthesis-viseme?tabs=visemeid&pivots=programming-language-csharp},
  note = {Accessed: 2024-06-26}
}

@book{Massaro1998,
  title={Perceiving Talking Faces: From Speech Perception to a Behavioral Principle},
  author={Massaro, Dominic W.},
  year={1998},
  publisher={MIT Press}
}
@book{Massaro1998,
  title={Perceiving Talking Faces: From Speech Perception to a Behavioral Principle},
  author={Massaro, Dominic W.},
  year={1998},
  publisher={MIT Press}
}
@book{Payack2008,
  title={A Million Words and Counting: How Global English Is Rewriting the World},
  author={Paul JJ Payack},
  year={2008},
  publisher={Citadel Press},
  address={New York},
}